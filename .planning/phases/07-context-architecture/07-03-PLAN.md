---
phase: 07-context-architecture
plan: 03
type: execute
wave: 3
depends_on: ["07-02"]
files_modified:
  - src/app/api/chat/route.ts
  - src/app/api/workshops/[workshopId]/steps/[stepId]/complete/route.ts
  - src/lib/ai/chat-config.ts
autonomous: true

must_haves:
  truths:
    - "AI receives prior step artifacts and summaries as context when processing each chat message"
    - "System prompt includes persistent memory and long-term memory tiers"
    - "Step completion triggers conversation summarization and saves summary to database"
    - "Context window stays manageable (hierarchical compression, not full history)"
  artifacts:
    - path: "src/app/api/chat/route.ts"
      provides: "Chat endpoint with three-tier context injection"
      exports: ["POST"]
    - path: "src/app/api/workshops/[workshopId]/steps/[stepId]/complete/route.ts"
      provides: "Step completion endpoint triggering summarization"
      exports: ["POST"]
    - path: "src/lib/ai/chat-config.ts"
      provides: "Updated system prompt builder with context injection"
      exports: ["buildStepSystemPrompt"]
  key_links:
    - from: "src/app/api/chat/route.ts"
      to: "src/lib/context/assemble-context.ts"
      via: "import and call assembleStepContext"
      pattern: "assembleStepContext"
    - from: "src/app/api/chat/route.ts"
      to: "src/lib/ai/chat-config.ts"
      via: "import and call buildStepSystemPrompt"
      pattern: "buildStepSystemPrompt"
    - from: "src/app/api/workshops/[workshopId]/steps/[stepId]/complete/route.ts"
      to: "src/lib/context/generate-summary.ts"
      via: "import and call generateStepSummary"
      pattern: "generateStepSummary"
    - from: "src/app/api/chat/route.ts"
      to: "streamText"
      via: "system prompt with context tiers"
      pattern: "system:.*buildStepSystemPrompt"
---

<objective>
Wire the context architecture into the existing chat API route and create a step completion endpoint that triggers conversation summarization.

Purpose: This is the integration layer that makes context architecture active. Without this plan, the schema and services from Plans 01-02 exist but nothing uses them. After this plan, every AI response is informed by prior step knowledge, and every step completion generates a summary for future steps.

Output: Updated chat route that injects three-tier context, new step completion API endpoint, and updated system prompt builder.
</objective>

<execution_context>
@/Users/michaelchristie/.claude/get-shit-done/workflows/execute-plan.md
@/Users/michaelchristie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/07-context-architecture/07-RESEARCH.md
@.planning/phases/07-context-architecture/07-01-SUMMARY.md
@.planning/phases/07-context-architecture/07-02-SUMMARY.md

@src/app/api/chat/route.ts
@src/lib/ai/chat-config.ts
@src/lib/ai/message-persistence.ts
@src/lib/context/assemble-context.ts
@src/lib/context/generate-summary.ts
@src/lib/context/types.ts
@src/lib/workshop/step-metadata.ts
@src/db/schema/steps.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update chat route to inject three-tier context</name>
  <files>
    src/app/api/chat/route.ts
    src/lib/ai/chat-config.ts
  </files>
  <action>
**src/lib/ai/chat-config.ts — Add buildStepSystemPrompt function:**

Keep the existing chatModel export unchanged. Replace the static SYSTEM_PROMPT string with a function that builds a context-aware system prompt.

```typescript
export function buildStepSystemPrompt(
  stepId: string,
  stepName: string,
  persistentContext: string,
  summaries: string
): string
```

Implementation:
1. Start with step-specific base prompt:
   ```
   You are an AI design thinking facilitator guiding the user through Step: {stepName}.
   Be encouraging, ask probing questions, and help them think deeply about their ideas.
   Keep responses concise and actionable.
   ```

2. If persistentContext is non-empty, add PERSISTENT MEMORY section:
   ```
   PERSISTENT MEMORY (Structured outputs from completed steps):
   {persistentContext}
   ```

3. If summaries is non-empty, add LONG-TERM MEMORY section:
   ```
   LONG-TERM MEMORY (Summaries of previous step conversations):
   {summaries}
   ```

4. Add instructions for context usage:
   ```
   CONTEXT USAGE RULES:
   - Reference prior step outputs by name when relevant (e.g., "Based on your HMW statement from the Challenge step...")
   - Build on prior knowledge — do not re-ask questions already answered in earlier steps
   - If the user's current input contradicts a prior step output, note the discrepancy gently
   ```

5. Return the complete prompt string

**Keep the old SYSTEM_PROMPT export** as a fallback (rename to GENERIC_SYSTEM_PROMPT). Export both the function and the generic constant.

**src/app/api/chat/route.ts — Wire in context assembly:**

Update the POST handler to:

1. Extract `workshopId` from the request body (in addition to existing messages, sessionId, stepId). The client must now send workshopId.
2. Validate workshopId is provided (add to existing validation)
3. Call assembleStepContext(workshopId, stepId, sessionId) to get the three-tier context
4. Look up step display name using getStepById(stepId) from step-metadata.ts
5. Call buildStepSystemPrompt(stepId, stepName, context.persistentContext, context.summaries) to build the context-aware system prompt
6. Pass the built system prompt to streamText instead of the static SYSTEM_PROMPT
7. Continue using context.messages is NOT needed here — the client already sends the current step's conversation messages. The short-term memory tier (current step verbatim) comes from the client's `messages` parameter.

**The change is specifically:**
- Before: `system: SYSTEM_PROMPT`
- After: `system: buildStepSystemPrompt(stepId, stepName, stepContext.persistentContext, stepContext.summaries)`

**Important considerations:**
- assembleStepContext is async — add await
- Context assembly adds latency. For Step 1 (no prior context), the function returns empty strings quickly. For Step 10, it queries up to 9 artifacts + 9 summaries. This is acceptable (small dataset, indexed queries).
- Do NOT change the streaming response format — keep toUIMessageStreamResponse with the same onFinish callback
- The system prompt should be >2,048 tokens by Step 3-4 (base prompt ~300 tokens + context grows with each step), enabling Gemini's automatic context caching for 90% cost reduction on the stable portion
  </action>
  <verify>
Run `npx tsc --noEmit` — TypeScript compilation succeeds.
Verify the chat route imports assembleStepContext and buildStepSystemPrompt.
Verify buildStepSystemPrompt conditionally includes memory sections only when non-empty.
Start dev server with `npm run dev` and verify it starts without errors.
  </verify>
  <done>Chat API route calls assembleStepContext on every request, builds a context-aware system prompt with prior step artifacts and summaries, and passes it to streamText. Step 1 requests work normally (empty context). Later step requests include prior step knowledge in the system prompt.</done>
</task>

<task type="auto">
  <name>Task 2: Create step completion endpoint with summary generation</name>
  <files>src/app/api/workshops/[workshopId]/steps/[stepId]/complete/route.ts</files>
  <action>
Create a POST endpoint that handles step completion. This endpoint:

1. Validates the request:
   - workshopId from URL params
   - stepId from URL params
   - sessionId from request body (needed to load conversation for summarization)

2. Finds the workshopStep record:
   - Query workshopSteps where workshopId = params.workshopId AND stepId = params.stepId
   - If not found, return 404

3. Updates workshopStep status to 'complete':
   - Set status = 'complete'
   - Set completedAt = new Date()
   - Use the existing updatedAt.$onUpdate() auto-update

4. Triggers summary generation (async, non-blocking to response):
   - Call generateStepSummary(sessionId, workshopStep.id, stepId, stepName) from context service
   - Use getStepById(stepId)?.name to get display name
   - Run this AFTER returning the response to avoid blocking the user. Use a fire-and-forget pattern:
     ```typescript
     // Don't await — let summarization happen in background
     generateStepSummary(sessionId, workshopStep.id, stepId, stepName)
       .catch(err => console.error('Summary generation failed:', err));
     ```
   - Note: In Vercel serverless, the function may terminate before this completes. To handle this, consider using `waitUntil` from Next.js if available, OR make the summarization synchronous (await it before responding). Since summaries are small (one API call), synchronous is acceptable — the user will wait ~1-2 seconds on step completion which is fine UX.
   - **Decision: Use synchronous approach (await before responding).** Reliability > speed for this operation. If summary fails, log error but still return success (step is complete regardless).

5. Returns 200 with: `{ success: true, stepId, status: 'complete' }`

**File path:** `src/app/api/workshops/[workshopId]/steps/[stepId]/complete/route.ts`

This follows Next.js App Router dynamic route conventions. The [workshopId] and [stepId] come from the URL path.

**Example request:**
```
POST /api/workshops/ws_abc123/steps/challenge/complete
Body: { "sessionId": "ses_xyz789" }
```

**Error handling:**
- 400 if sessionId missing
- 404 if workshopStep not found
- 500 with generic error message if something else fails
- Summary generation failure: log error, still return 200 (step completion succeeded)

**Auth:** Skip auth check for now — will be added when step-specific permissions are implemented. Add a TODO comment.
  </action>
  <verify>
Run `npx tsc --noEmit` — TypeScript compilation succeeds.
Verify the file exists at the correct Next.js dynamic route path.
Start dev server with `npm run dev` — verify it starts without errors.
Test with curl: `curl -X POST http://localhost:3000/api/workshops/test/steps/test/complete -H "Content-Type: application/json" -d '{"sessionId":"test"}' ` — should return 404 (no matching workshopStep) rather than a crash.
  </verify>
  <done>Step completion endpoint exists at /api/workshops/[workshopId]/steps/[stepId]/complete. It updates workshopStep status to 'complete', generates a conversation summary via AI, and saves it to step_summaries. Summary generation failure does not block step completion.</done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with no errors
2. `npm run dev` starts without errors
3. Chat route uses assembleStepContext to inject prior step knowledge
4. System prompt conditionally includes PERSISTENT MEMORY and LONG-TERM MEMORY sections
5. Step completion endpoint triggers summarization
6. End-to-end flow: Chat at Step 1 -> Complete Step 1 -> Summary generated -> Chat at Step 2 -> System prompt includes Step 1 summary and any artifacts
</verification>

<success_criteria>
- Every chat request at Step 2+ includes prior step context in the system prompt
- Step completion generates a conversation summary stored in step_summaries
- System prompt exceeds 2,048 tokens by Step 3-4 (enabling Gemini context caching)
- Step 1 chat works normally without errors (empty context case handled)
- Existing chat functionality is preserved (streaming, message persistence, error handling)
</success_criteria>

<output>
After completion, create `.planning/phases/07-context-architecture/07-03-SUMMARY.md`
</output>

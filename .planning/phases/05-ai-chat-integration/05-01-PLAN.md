---
phase: 05-ai-chat-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - package.json
  - src/db/schema/chat-messages.ts
  - src/db/schema/index.ts
  - src/db/schema/relations.ts
  - src/lib/ai/chat-config.ts
  - src/lib/ai/message-persistence.ts
  - src/app/api/chat/route.ts
autonomous: true
user_setup:
  - service: google-gemini
    why: "AI chat responses via Gemini API"
    env_vars:
      - name: GOOGLE_GENERATIVE_AI_API_KEY
        source: "Google AI Studio (https://aistudio.google.com/apikey) -> Create API key"

must_haves:
  truths:
    - "Chat messages table exists in database with session and step scoping"
    - "POST /api/chat accepts messages and returns streaming Gemini response"
    - "Messages persist to database after streaming completes"
  artifacts:
    - path: "src/db/schema/chat-messages.ts"
      provides: "chatMessages table definition"
      contains: "chatMessages"
    - path: "src/app/api/chat/route.ts"
      provides: "Streaming chat endpoint"
      exports: ["POST"]
    - path: "src/lib/ai/message-persistence.ts"
      provides: "saveMessages and loadMessages functions"
      exports: ["saveMessages", "loadMessages"]
    - path: "src/lib/ai/chat-config.ts"
      provides: "Gemini model configuration"
      exports: ["chatModel"]
  key_links:
    - from: "src/app/api/chat/route.ts"
      to: "src/lib/ai/chat-config.ts"
      via: "imports chatModel"
      pattern: "import.*chatModel.*chat-config"
    - from: "src/app/api/chat/route.ts"
      to: "src/lib/ai/message-persistence.ts"
      via: "onFinish callback saves messages"
      pattern: "saveMessages"
    - from: "src/lib/ai/message-persistence.ts"
      to: "src/db/schema/chat-messages.ts"
      via: "drizzle insert/select on chatMessages"
      pattern: "chatMessages"
---

<objective>
Install AI SDK dependencies, create the chat_messages database table, build the streaming chat API route handler, and implement message persistence logic.

Purpose: This plan establishes the entire backend infrastructure for AI chat -- the database schema for storing conversations, the Gemini API integration via Vercel AI SDK, and the persistence layer that saves/loads messages per session and step. Plan 02 will wire the frontend to this backend.

Output: Working /api/chat endpoint that streams Gemini responses, chat_messages table in database, persistence functions for save/load.
</objective>

<execution_context>
@/Users/michaelchristie/.claude/get-shit-done/workflows/execute-plan.md
@/Users/michaelchristie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-ai-chat-integration/05-RESEARCH.md

# Existing code to understand patterns
@src/db/client.ts
@src/db/schema/index.ts
@src/db/schema/sessions.ts
@src/db/schema/steps.ts
@src/db/schema/relations.ts
@src/lib/ids.ts
@package.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install AI SDK packages and create chat_messages schema</name>
  <files>
    package.json
    src/db/schema/chat-messages.ts
    src/db/schema/index.ts
    src/db/schema/relations.ts
  </files>
  <action>
1. Install AI SDK packages:
   ```
   npm install ai @ai-sdk/react @ai-sdk/google
   ```
   Note: zod is already installed. Do NOT install it again.

2. Create `src/db/schema/chat-messages.ts` following existing schema patterns (see sessions.ts, steps.ts for conventions):
   - Table name: `chat_messages`
   - id: text primary key with `createPrefixedId('msg')` default (follow existing prefix pattern from ids.ts)
   - sessionId: text, notNull, references sessions.id with onDelete cascade
   - stepId: text, notNull (stores the step definition ID like 'empathize', 'define' -- this is a semantic string, NOT a foreign key to workshopSteps.id)
   - messageId: text, notNull (the UIMessage.id from AI SDK, used for deduplication)
   - role: text, notNull (values: 'user' | 'assistant' | 'system')
   - content: text, notNull (the text content of the message)
   - createdAt: timestamp with mode 'date', precision 3, notNull, defaultNow
   - Composite index on (sessionId, stepId) named 'chat_messages_session_step_idx'
   - Index on messageId named 'chat_messages_message_id_idx' for dedup lookups

   IMPORTANT: stepId here stores step definition IDs like 'empathize', 'define', NOT workshop_steps.id. This matches how the frontend identifies steps. Do NOT add a foreign key to workshopSteps -- it would require looking up the workshopStep record for every chat message, which is unnecessary.

3. Export from `src/db/schema/index.ts`: Add `export * from './chat-messages';`

4. Add chatMessages relations to `src/db/schema/relations.ts`:
   - chatMessages belongs to session (chatMessages.sessionId -> sessions.id)
   - Add chatMessages to sessionsRelations as `many(chatMessages)`

5. Push schema to database:
   ```
   npm run db:push:dev
   ```
  </action>
  <verify>
    - `npm run build` completes without TypeScript errors
    - `npm run db:push:dev` succeeds (schema pushed to Neon)
    - The chatMessages export is available from `@/db/schema`
  </verify>
  <done>chat_messages table exists in database with correct columns, indexes, and relations. Schema exports work.</done>
</task>

<task type="auto">
  <name>Task 2: Create Gemini config, message persistence, and streaming chat route</name>
  <files>
    src/lib/ai/chat-config.ts
    src/lib/ai/message-persistence.ts
    src/app/api/chat/route.ts
  </files>
  <action>
1. Create `src/lib/ai/chat-config.ts`:
   - Import `google` from `@ai-sdk/google`
   - Export `chatModel` as `google('gemini-2.0-flash')` -- fast, cost-effective for MVP
   - Export `SYSTEM_PROMPT` as a generic design thinking facilitator prompt:
     "You are a helpful design thinking facilitator. Guide the user through the current step of the design thinking process. Be encouraging, ask probing questions, and help them think deeply about their ideas. Keep responses concise and actionable."
   - This is a generic prompt for all steps (step-specific prompts come in MVP 1.0)

2. Create `src/lib/ai/message-persistence.ts`:
   - Import `db` from `@/db/client`, `chatMessages` from `@/db/schema`, `eq`, `and`, `asc` from `drizzle-orm`
   - Import `UIMessage` type from `ai`

   `saveMessages(sessionId: string, stepId: string, messages: UIMessage[])`:
   - Iterate over messages array
   - For each message, extract text parts (filter `part.type === 'text'`)
   - Concatenate all text parts into single content string
   - Use `db.insert(chatMessages).values({...}).onConflictDoNothing()` with a check -- use messageId for deduplication
   - Actually simpler: delete existing messages for this session+step, then bulk insert all messages. This handles the case where AI SDK sends the complete message array in onFinish.
   - CORRECTION: Better approach -- insert only NEW messages. Compare incoming message IDs with existing. Use a transaction:
     a. Fetch existing messageIds for this session+step
     b. Filter incoming messages to only those not already saved
     c. Insert new messages

   `loadMessages(sessionId: string, stepId: string): Promise<UIMessage[]>`:
   - Query chatMessages where sessionId AND stepId match, ordered by createdAt asc
   - Group by messageId (multiple parts may exist -- though for MVP we store one row per message with concatenated text)
   - Map to UIMessage format: `{ id: msg.messageId, role: msg.role as 'user' | 'assistant', parts: [{ type: 'text', text: msg.content }], createdAt: msg.createdAt }`
   - Return the array

3. Create `src/app/api/chat/route.ts`:
   - Import `streamText`, `convertToModelMessages` from `ai`
   - Import `chatModel`, `SYSTEM_PROMPT` from `@/lib/ai/chat-config`
   - Import `saveMessages` from `@/lib/ai/message-persistence`

   - Export `const maxDuration = 30` (increase Vercel serverless timeout for AI responses)

   - Export async function POST(req: Request):
     a. Parse request body: `const { messages, sessionId, stepId } = await req.json()`
     b. Validate that sessionId and stepId are present (return 400 if missing)
     c. Call `streamText({ model: chatModel, system: SYSTEM_PROMPT, messages: convertToModelMessages(messages) })`
     d. Call `result.consumeStream()` to ensure stream completes even if client disconnects (critical for persistence)
     e. Return `result.toUIMessageStreamResponse({ sendReasoning: false, onFinish: async ({ messages: responseMessages }) => { await saveMessages(sessionId, stepId, responseMessages); } })`

   IMPORTANT: Use `convertToModelMessages` (NOT passing messages directly) to convert UIMessage format to CoreMessage format that Gemini expects. This is a common pitfall from the research.

   IMPORTANT: Call `result.consumeStream()` BEFORE returning the response. This ensures the stream completes server-side even if the client disconnects, so onFinish always fires and messages persist.
  </action>
  <verify>
    - `npm run build` completes without errors
    - The /api/chat route file exists and exports POST
    - chat-config.ts exports chatModel and SYSTEM_PROMPT
    - message-persistence.ts exports saveMessages and loadMessages
  </verify>
  <done>Streaming chat API route accepts POST requests with messages/sessionId/stepId, streams Gemini responses via AI SDK, and persists completed messages to database via onFinish callback. Message persistence functions can save and load messages scoped by session and step.</done>
</task>

</tasks>

<verification>
1. `npm run build` passes with zero errors
2. Schema is pushed to database (`npm run db:push:dev` succeeded)
3. All new files exist and export correctly:
   - `src/db/schema/chat-messages.ts` exports chatMessages
   - `src/lib/ai/chat-config.ts` exports chatModel, SYSTEM_PROMPT
   - `src/lib/ai/message-persistence.ts` exports saveMessages, loadMessages
   - `src/app/api/chat/route.ts` exports POST, maxDuration
</verification>

<success_criteria>
- AI SDK packages installed (ai, @ai-sdk/react, @ai-sdk/google in package.json)
- chat_messages table created in Neon database with session+step composite index
- /api/chat POST endpoint exists and compiles
- Message persistence layer can save and load UIMessage arrays
- Build passes with no TypeScript errors
</success_criteria>

<output>
After completion, create `.planning/phases/05-ai-chat-integration/05-01-SUMMARY.md`
</output>

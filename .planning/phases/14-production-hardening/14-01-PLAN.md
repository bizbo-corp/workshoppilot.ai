---
phase: 14-production-hardening
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/lib/ai/gemini-retry.ts
  - src/app/api/chat/route.ts
  - src/lib/extraction/extract-artifact.ts
  - src/lib/context/generate-summary.ts
  - src/app/api/health/route.ts
  - vercel.json
  - package.json
autonomous: true
user_setup:
  - service: vercel-cron
    why: "Health check warming to prevent Neon cold starts"
    env_vars:
      - name: CRON_SECRET
        source: "Generate a random secret (e.g., openssl rand -base64 32) and add to both .env.local and Vercel project settings"
    dashboard_config:
      - task: "Add CRON_SECRET environment variable"
        location: "Vercel Dashboard -> Project Settings -> Environment Variables"

must_haves:
  truths:
    - "Gemini 429 rate limit errors are retried with exponential backoff and jitter instead of failing immediately"
    - "All three Gemini call sites (chat streaming, artifact extraction, summary generation) use the retry wrapper"
    - "Non-rate-limit errors are NOT retried (fail fast)"
    - "Health check endpoint authenticates via CRON_SECRET and rejects unauthorized requests"
    - "Vercel cron pings /api/health every 4 minutes to keep Neon compute active"
  artifacts:
    - path: "src/lib/ai/gemini-retry.ts"
      provides: "Rate limit retry wrappers for streamText and generateText"
      exports: ["streamTextWithRetry", "generateTextWithRetry", "isGeminiRateLimitError"]
    - path: "vercel.json"
      provides: "Cron configuration for health check warming"
      contains: "*/4 * * * *"
  key_links:
    - from: "src/app/api/chat/route.ts"
      to: "src/lib/ai/gemini-retry.ts"
      via: "import streamTextWithRetry"
      pattern: "streamTextWithRetry"
    - from: "src/lib/extraction/extract-artifact.ts"
      to: "src/lib/ai/gemini-retry.ts"
      via: "import streamTextWithRetry for extraction"
      pattern: "streamTextWithRetry"
    - from: "src/lib/context/generate-summary.ts"
      to: "src/lib/ai/gemini-retry.ts"
      via: "import generateTextWithRetry for summaries"
      pattern: "generateTextWithRetry"
---

<objective>
Rate limit retry infrastructure and Neon cold start prevention

Purpose: Wrap all Gemini API calls in exponential backoff with jitter to handle 429 errors gracefully, and configure Vercel cron to keep Neon database active during user hours. This is the backend foundation that Plan 02 builds UI on top of.

Output: gemini-retry.ts library, updated API routes with retry wrappers, authenticated health endpoint, vercel.json cron config
</objective>

<execution_context>
@/Users/michaelchristie/.claude/get-shit-done/workflows/execute-plan.md
@/Users/michaelchristie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-production-hardening/14-RESEARCH.md
@src/app/api/chat/route.ts
@src/lib/extraction/extract-artifact.ts
@src/lib/context/generate-summary.ts
@src/app/api/health/route.ts
@src/lib/ai/chat-config.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Gemini retry wrapper library and wire into all API call sites</name>
  <files>
    src/lib/ai/gemini-retry.ts
    src/app/api/chat/route.ts
    src/lib/extraction/extract-artifact.ts
    src/lib/context/generate-summary.ts
    package.json
  </files>
  <action>
1. Install exponential-backoff: `npm install exponential-backoff`

2. Create `src/lib/ai/gemini-retry.ts` with:
   - `isGeminiRateLimitError(error: unknown): boolean` — checks for HTTP 429 status, RESOURCE_EXHAUSTED code, and "rate limit"/"quota exceeded" in error messages
   - Backoff config object: startingDelay 1000ms, timeMultiple 2, numOfAttempts 5, maxDelay 15000ms, jitter 'full'
   - `streamTextWithRetry(config: Parameters<typeof streamText>[0]): Promise<ReturnType<typeof streamText>>` — wraps `streamText` from 'ai' with `backOff` from 'exponential-backoff'. On rate limit errors, rethrow for retry. On non-rate-limit errors, return false from retry callback to fail immediately. Log retry attempts with `console.warn`.
   - `generateTextWithRetry(config: Parameters<typeof generateText>[0]): Promise<ReturnType<typeof generateText>>` — same pattern wrapping `generateText` for summary generation.
   - Export all three functions (isGeminiRateLimitError, streamTextWithRetry, generateTextWithRetry).

   IMPORTANT: The retry callback's return value (true/false) controls whether to retry. Use a sentinel error message 'NON_RETRYABLE' to distinguish. The `backOff` function from exponential-backoff takes an async function and options object — the `retry` option is a callback `(error, attemptNumber) => boolean`.

3. Modify `src/app/api/chat/route.ts`:
   - Import `streamTextWithRetry` and `isGeminiRateLimitError` from `@/lib/ai/gemini-retry`
   - Replace `streamText({...})` call (line ~68) with `streamTextWithRetry({...})`
   - Add a catch block specifically for rate limit errors after all retries are exhausted: return JSON response with `{ error: 'rate_limit_exceeded', message: 'The AI is currently experiencing high demand. Please try again in a few moments.', retryAfter: 30 }`, status 429, with `Retry-After: 30` header
   - Keep existing generic error catch for non-rate-limit errors (500)

4. Modify `src/lib/extraction/extract-artifact.ts`:
   - Import `streamTextWithRetry` from `@/lib/ai/gemini-retry`
   - Replace the `streamText({...})` call (line ~78) with `streamTextWithRetry({...})`
   - The existing extraction retry loop (for Zod validation failures) stays as-is — the rate limit retry is a separate concern at the API level. This means: extraction retries for schema validation, AND each attempt has rate limit retries built in.

5. Modify `src/lib/context/generate-summary.ts`:
   - Import `generateTextWithRetry` from `@/lib/ai/gemini-retry`
   - Replace `generateText({...})` call (line ~50) with `generateTextWithRetry({...})`
   - The existing fallback (save error message as summary) remains for non-rate-limit failures.
  </action>
  <verify>
    - `npm run build` passes (TypeScript compiles without errors)
    - `grep -r "streamTextWithRetry" src/` shows imports in chat route and extract-artifact
    - `grep -r "generateTextWithRetry" src/` shows import in generate-summary
    - `grep -r "exponential-backoff" src/lib/ai/gemini-retry.ts` confirms library usage
  </verify>
  <done>
    All three Gemini call sites (chat, extraction, summary) use retry wrappers. Rate limit errors trigger up to 5 attempts with exponential backoff and full jitter. Non-rate-limit errors fail immediately. Chat API returns structured 429 response when all retries exhausted.
  </done>
</task>

<task type="auto">
  <name>Task 2: Configure health check authentication and Vercel cron warming</name>
  <files>
    src/app/api/health/route.ts
    vercel.json
  </files>
  <action>
1. Modify `src/app/api/health/route.ts`:
   - Add CRON_SECRET authentication check at the start of GET handler
   - Check `req.headers.get('authorization')` against `Bearer ${process.env.CRON_SECRET}`
   - BUT: Allow unauthenticated requests to still work (return health status) — only REQUIRE auth when CRON_SECRET env var is set. This way the endpoint works in development without the secret, and monitoring tools can still ping it. Pattern:
     ```
     const cronSecret = process.env.CRON_SECRET;
     if (cronSecret) {
       const authHeader = req.headers.get('authorization');
       if (authHeader !== `Bearer ${cronSecret}`) {
         return new Response('Unauthorized', { status: 401 });
       }
     }
     ```
   - Change the function signature from `GET()` to `GET(req: Request)` to accept the request parameter
   - Add a `warmedAt` timestamp to the response body for monitoring: `{ status: 'healthy', database: 'connected', timestamp: new Date().toISOString(), warmedAt: new Date().toISOString() }`
   - Keep the existing 5-second timeout race pattern

2. Create `vercel.json` at project root (this file does NOT exist yet):
   ```json
   {
     "crons": [
       {
         "path": "/api/health",
         "schedule": "*/4 * * * *"
       }
     ]
   }
   ```
   Note: Vercel cron automatically sends `Authorization: Bearer <CRON_SECRET>` when CRON_SECRET env var is configured in Vercel project settings. The 4-minute interval keeps Neon active (5-min scale-to-zero timeout).
  </action>
  <verify>
    - `npm run build` passes
    - `vercel.json` exists at project root with cron configuration
    - Health endpoint code contains auth check pattern with `CRON_SECRET`
  </verify>
  <done>
    Health check endpoint authenticates cron requests via CRON_SECRET (optional in dev, required in prod when env var is set). vercel.json configures 4-minute cron to prevent Neon cold starts. Unauthenticated requests return 401 when CRON_SECRET is configured.
  </done>
</task>

</tasks>

<verification>
1. `npm run build` completes without TypeScript errors
2. `exponential-backoff` appears in package.json dependencies
3. All three Gemini call sites use retry wrappers (chat route, extract-artifact, generate-summary)
4. Health endpoint has auth guard
5. vercel.json has cron config at 4-minute interval
6. Chat API returns structured 429 response (not generic 500) when rate limited
</verification>

<success_criteria>
- exponential-backoff installed and used in gemini-retry.ts
- Chat route, extraction, and summary generation all use retry wrappers
- 429 errors trigger exponential backoff with full jitter (up to 5 attempts)
- Non-rate-limit errors fail immediately (not retried)
- Health endpoint requires CRON_SECRET auth in production
- vercel.json cron pings /api/health every 4 minutes
- Build passes cleanly
</success_criteria>

<output>
After completion, create `.planning/phases/14-production-hardening/14-01-SUMMARY.md`
</output>

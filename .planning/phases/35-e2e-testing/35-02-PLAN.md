---
phase: 35-e2e-testing
plan: 02
type: execute
wave: 2
depends_on: ["35-01"]
files_modified:
  - e2e/workshop-walkthrough.spec.ts
autonomous: true

must_haves:
  truths:
    - "Playwright test creates a fresh workshop and navigates through all 10 steps"
    - "Each step receives an AI greeting, accepts user input, and AI responds meaningfully"
    - "Step advancement via Next button works for all 10 steps sequentially"
    - "Test validates AI responses are non-empty and contextually relevant"
    - "Canvas-enabled steps show canvas content after AI interaction"
  artifacts:
    - path: "e2e/workshop-walkthrough.spec.ts"
      provides: "Full 10-step E2E test walking through workshop creation to completion"
      contains: "test.*step"
  key_links:
    - from: "e2e/workshop-walkthrough.spec.ts"
      to: "e2e/helpers/workshop-factory.ts"
      via: "import helpers"
      pattern: "createWorkshopViaUI|sendMessage|clickNextStep"
    - from: "e2e/workshop-walkthrough.spec.ts"
      to: "/api/chat"
      via: "Playwright browser sends messages to real Gemini API"
      pattern: "sendMessage"
---

<objective>
Create the full E2E workshop walkthrough test that creates a fresh workshop, walks forward through all 10 steps with real AI (Gemini) interaction, validates responses and step transitions. Happy path only — forward progression, no back-revise scenarios.

Purpose: This is the core E2E test — validates the entire workshop flow end-to-end with real AI, real database, and real browser interactions. Note: The user performs manual testing separately (before or alongside automated testing) to surface issues that automated tests might miss. The test-and-fix loop in Task 2 addresses bugs discovered from BOTH manual user testing and automated Playwright runs.
Output: A single comprehensive Playwright spec file testing the happy-path forward progression through all 10 workshop steps.
</objective>

<execution_context>
@/Users/michaelchristie/.claude/get-shit-done/workflows/execute-plan.md
@/Users/michaelchristie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/35-e2e-testing/35-01-SUMMARY.md
@src/lib/workshop/step-metadata.ts
@src/components/workshop/chat-panel.tsx
@src/components/workshop/step-navigation.tsx
@src/app/workshop/[sessionId]/step/[stepId]/page.tsx
@src/proxy.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create the full 10-step workshop walkthrough E2E test</name>
  <files>
    e2e/workshop-walkthrough.spec.ts
  </files>
  <action>
    Create a Playwright spec file that tests a complete workshop walkthrough from creation to Step 10 completion. Uses real Gemini AI (no mocks), fresh workshop (not seed data), happy path only.

    **Test structure — single test with serial steps:**

    Use `test.describe.serial` with a shared `page` fixture to maintain state across steps. The workshop is created once at the beginning, and each step builds on the previous.

    ```
    test.describe.serial('Workshop Walkthrough', () => {
      let page: Page;
      let sessionId: string;
      let workshopId: string; // extracted from page context

      test.beforeAll(async ({ browser }) => {
        page = await browser.newPage();
      });

      test.afterAll(async () => {
        await page.close();
      });

      test('Create workshop and land on Step 1', ...);
      test('Step 1: Challenge', ...);
      test('Step 2: Stakeholder Mapping', ...);
      // ... through Step 10
    });
    ```

    **Workshop creation test:**
    1. Navigate to `'/'`
    2. Click the "Start Workshop" button (use `page.getByRole('link', { name: /start workshop/i })` or `page.getByText('Start Workshop')`)
    3. Wait for URL to match `/workshop/.*/step/1`
    4. Extract `sessionId` from URL pattern `/workshop/([^/]+)/step/`
    5. Wait for the AI greeting to appear (first assistant message bubble — look for the AI avatar div with text "AI" followed by a `.bg-muted` message div)
    6. Assert: page URL contains `/step/1`, AI greeting is visible

    **Per-step test pattern (Steps 1-9):**
    Each step test should:
    1. Verify we are on the correct step URL (`/step/{N}`)
    2. Wait for AI greeting/initial message (assistant message bubble visible)
    3. Count existing assistant messages
    4. Type a contextual message into the textarea (`page.locator('textarea')`) and click the send button (`page.getByRole('button', { name: 'Send message' })`)
    5. Wait for AI response: wait until a NEW assistant message appears (count increases). Use `page.locator('.bg-muted.rounded-lg').nth(expectedIndex)` or wait for text content to change. Timeout: 90 seconds for AI response.
    6. Assert: AI response is non-empty (has text content)
    7. Click "Next" or "Skip to Next" button: `page.getByRole('button', { name: /next|skip to next/i })`
    8. Wait for URL to change to `/step/{N+1}` with `page.waitForURL`

    **Step 10 (last step):**
    Same as above but do NOT click Next (there is no Next button on step 10). Instead, verify the step loaded and AI responded.

    **Contextual messages per step (realistic user inputs):**
    - Step 1 (Challenge): "I want to build a pet care app that helps busy pet owners manage their pets' daily routines and health needs"
    - Step 2 (Stakeholder Mapping): "The main stakeholders are pet owners, veterinarians, pet sitters, and pet supply companies"
    - Step 3 (User Research): "I'd like to interview busy professionals who own multiple pets and struggle with keeping up with vet appointments and feeding schedules"
    - Step 4 (Sense Making): "The key themes I see are time management struggles, anxiety about pet health, and desire for automated reminders"
    - Step 5 (Persona): "Based on our research, the primary persona would be a working professional aged 28-35 who owns 2-3 pets"
    - Step 6 (Journey Mapping): "The user's journey starts when they wake up and need to manage morning feeding, then continues through their workday worrying about their pets"
    - Step 7 (Reframe): "I'd like to focus the HMW on making pet care routine management effortless for multi-pet owners"
    - Step 8 (Ideation): "Some initial ideas: smart feeding schedule optimizer, vet appointment coordinator with reminders, pet health dashboard, community care sharing platform"
    - Step 9 (Concept): "Let's develop the smart feeding schedule optimizer concept further with SWOT analysis"
    - Step 10 (Validate): "Let's create a user flow for the core feeding schedule feature and outline the key screens"

    **Timeouts:**
    - AI greeting wait: 60 seconds (cold start possible)
    - AI response wait: 90 seconds (Gemini can be slow)
    - Step navigation wait: 30 seconds (server action + page load)
    - Overall test timeout: 20 minutes (configured in the test file via `test.setTimeout(1200000)`)

    **Selector strategy:**
    - Chat textarea: `page.locator('textarea[placeholder*="Type your message"]')`
    - Send button: `page.getByRole('button', { name: 'Send message' })`
    - AI messages: `page.locator('.rounded-full.bg-primary').locator('..').locator('..').locator('.bg-muted')` — the assistant message bubbles are siblings of the "AI" avatar circle. Simpler approach: count elements matching `.rounded-lg.bg-muted.p-3` within the chat scroll area.
    - Next button: `page.getByRole('button', { name: /Next|Skip to Next/i })`
    - Typing indicator: `page.getByText('AI is thinking...')`

    **Important implementation notes:**
    - Import `test, expect, type Page` from `@playwright/test`
    - Use `test.slow()` at the describe level since this is a long-running test with real AI
    - After sending a message, wait for the typing indicator to appear and then disappear, OR wait for assistant message count to increase
    - The `__step_start__` auto-trigger in chat-panel.tsx sends an initial message automatically when entering a step. The test should wait for this AI greeting before interacting.
    - Between step transitions, add a small `page.waitForTimeout(2000)` to let the new step page fully hydrate before interacting
    - If the AI response takes too long, the test will fail with a clear timeout message — this is expected behavior since we use real Gemini API
  </action>
  <verify>
    - `cat e2e/workshop-walkthrough.spec.ts` exists with 10 step tests
    - `npx playwright test --list` shows the test file with step-named tests
    - TypeScript compiles: `npx tsc --noEmit --project tsconfig.json` (may need to adjust tsconfig to include e2e/)
  </verify>
  <done>Full 10-step workshop walkthrough E2E test created with real AI interaction, contextual messages per step, proper waits for streaming responses, and step-by-step assertions</done>
</task>

<task type="auto">
  <name>Task 2: Run E2E test, fix discovered bugs, and verify passage</name>
  <files>
    e2e/workshop-walkthrough.spec.ts
    (plus any source files where bugs are found)
  </files>
  <action>
    Run the E2E test and fix any bugs discovered during the run. This is the critical test-and-fix loop. Bugs may come from two sources: (1) the user's manual walkthrough testing (reported as issues to fix), and (2) automated Playwright test failures. Both are addressed here.

    **Execution:**
    1. Start the dev server with auth bypass: `BYPASS_AUTH=true npm run dev` (in background or via Playwright webServer config)
    2. Run the test: `npm run test:e2e -- --headed` (headed mode to observe first run)
    3. If test fails at a specific step:
       a. Read the Playwright trace/screenshot from `test-results/`
       b. Diagnose: Is it a selector issue? Timing issue? Actual bug?
       c. Fix the issue:
          - **Selector wrong:** Update the selector in the test file
          - **Timing issue:** Increase wait time or add explicit waits
          - **Actual app bug (small):** Fix inline and note it for the test report
          - **Actual app bug (large):** Add a workaround in the test and note it as deferred
       d. Re-run the test
    4. Repeat until the test passes all 10 steps or all remaining failures are documented deferrals

    **Common failure modes to handle:**
    - Auth redirect: BYPASS_AUTH not taking effect → check middleware bypass implementation
    - AI timeout: Gemini takes >90s → increase timeout or add retry logic
    - Step page not loading: Server component error → check console for errors
    - Canvas not rendering: React Flow or Konva hydration issue → add wait for canvas container
    - Next button disabled: Artifact not confirmed → may need to interact more or wait for extraction

    **Bug fix commit convention:**
    - Batch all fixes for a given area into one commit
    - Message format: `fix(35-02): [what was fixed]`

    **Quality bar:** Demo-ready. The test should complete without crashes. Minor visual issues are acceptable as long as the core flow works.
  </action>
  <verify>
    - `npm run test:e2e` passes (or documents specific deferred failures)
    - No app crashes during the test run
    - Playwright HTML report: `npx playwright show-report` displays results
  </verify>
  <done>E2E test passes through all 10 steps (or has documented deferrals), discovered bugs are fixed inline, test results saved</done>
</task>

</tasks>

<verification>
1. `npm run test:e2e` completes without crashing
2. Test covers all 10 steps: Challenge through Validate
3. Each step test sends a user message and receives an AI response
4. Step transitions work from Step 1 through Step 10
5. Playwright report shows test results: `npx playwright show-report`
</verification>

<success_criteria>
A Playwright E2E test exercises the complete workshop flow from creation through all 10 steps with real AI interaction. The test passes or has documented deferrals for any issues requiring larger fixes.
</success_criteria>

<output>
After completion, create `.planning/phases/35-e2e-testing/35-02-SUMMARY.md`
</output>

---
phase: 11-discovery-steps-1-4
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/lib/ai/prompts/step-prompts.ts
  - src/lib/ai/prompts/validation-criteria.ts
autonomous: true

must_haves:
  truths:
    - "Step 1 AI explicitly checks altitude and pushes back on solutions disguised as problems"
    - "Step 2 AI proactively prompts for missing stakeholder categories (Core, Direct, Indirect)"
    - "Step 3 AI facilitates synthetic interviews by roleplaying stakeholders from Step 2"
    - "Step 4 AI requires evidence traceability from every pain/gain back to Step 3 findings"
    - "Steps 1-3 prompts explicitly avoid premature synthesis"
  artifacts:
    - path: "src/lib/ai/prompts/step-prompts.ts"
      provides: "Enriched prompts for Steps 1-4 with domain-specific guidance"
      contains: "altitude"
    - path: "src/lib/ai/prompts/validation-criteria.ts"
      provides: "Enhanced validation criteria for Steps 1-4"
      contains: "Altitude Check"
  key_links:
    - from: "src/lib/ai/prompts/step-prompts.ts"
      to: "src/lib/ai/chat-config.ts"
      via: "getStepSpecificInstructions called by buildStepSystemPrompt"
      pattern: "getStepSpecificInstructions"
    - from: "src/lib/ai/prompts/validation-criteria.ts"
      to: "src/lib/ai/chat-config.ts"
      via: "getValidationCriteria called during Validate phase"
      pattern: "getValidationCriteria"
---

<objective>
Enrich step prompts and validation criteria for Steps 1-4 with design thinking domain expertise from Obsidian specs and research.

Purpose: Current prompts are functional but generic. Steps 1-4 need specific facilitation guidance: altitude checking (Step 1), stakeholder category prompting (Step 2), synthetic interview facilitation (Step 3), and evidence-grounded clustering (Step 4). These enrichments transform the AI from a generic facilitator to a domain expert.

Output: Step-prompts.ts and validation-criteria.ts updated with detailed instructions for Steps 1-4.
</objective>

<execution_context>
@/Users/michaelchristie/.claude/get-shit-done/workflows/execute-plan.md
@/Users/michaelchristie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/11-discovery-steps-1-4/11-RESEARCH.md
@src/lib/ai/prompts/step-prompts.ts
@src/lib/ai/prompts/validation-criteria.ts
@src/lib/schemas/step-schemas.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Enrich step prompts for Steps 1-4</name>
  <files>src/lib/ai/prompts/step-prompts.ts</files>
  <action>
Update `getStepSpecificInstructions` for steps 'challenge', 'stakeholder-mapping', 'user-research', and 'sense-making'. Keep the existing structure (STEP GOAL, DESIGN THINKING PRINCIPLES, GATHERING REQUIREMENTS, PRIOR CONTEXT USAGE) but significantly enrich each section. Do NOT modify Steps 5-10 prompts.

**Step 1: 'challenge' -- Add:**
- ALTITUDE CHECKING section: "Draft 3 HMW variants at different altitudes (specific/balanced/broad) and explain tradeoffs. Recommend which altitude best fits the user's goals."
- ANTI-PATTERNS section: "If user describes a solution ('I want an app that...'), redirect: 'That sounds like a solution. What is the underlying pain point?' If user gives a vision statement ('fix education'), narrow: 'Who specifically experiences this problem? What does their day look like?'"
- Update GATHERING REQUIREMENTS to include: "Draft 3 HMW variants and ask user to select or refine"
- Add: "BOUNDARY: This step is about DEFINING the problem, not solving it. Do not suggest solutions or features."

**Step 2: 'stakeholder-mapping' -- Add:**
- PROACTIVE PROMPTING section: "After initial stakeholder brainstorm, proactively check for gaps: 'You've identified [users]. What about decision-makers who approve this? Funders? Regulators? Internal team members who would build or maintain this? Partners or vendors?' Use the challenge context to suggest domain-specific stakeholders."
- CATEGORY CHECKLIST: "Ensure coverage across: Users, Buyers/Decision-makers, Influencers, Regulators/Compliance, Internal Team, Partners/Vendors. If a category is empty, ask about it explicitly."
- Update GATHERING REQUIREMENTS: For each stakeholder, gather power level (high/medium/low), interest level (high/medium/low), and brief notes about their perspective
- Add: "BOUNDARY: This step is about MAPPING stakeholders, not researching them. Do not generate interview questions or insights yet -- that is Step 3."

**Step 3: 'user-research' -- Add:**
- SYNTHETIC INTERVIEW FLOW section:
  ```
  SYNTHETIC INTERVIEW FACILITATION:
  1. Generate 3-5 open-ended interview questions based on challenge (Step 1) and stakeholder map (Step 2)
  2. Present questions to user for approval/modification
  3. For each Core stakeholder from Step 2, simulate a synthetic interview:
     - Roleplay as that stakeholder using their name, role, power/interest levels, and notes
     - Answer interview questions from their realistic perspective
     - Include specific examples, frustrations with current solutions, and desired outcomes
     - Express hesitation or uncertainty where realistic -- real people are messy
  4. After each synthetic interview, capture key insights with source attribution
  5. Offer alternative: "Would you like to paste real interview transcripts or research data instead?"

  SYNTHETIC INTERVIEW QUALITY:
  - Use temperature guidance in prompts: be creative and realistic, not formulaic
  - Each stakeholder should sound DIFFERENT (different priorities, different frustrations)
  - Include concrete details specific to the challenge domain
  - Mention specific tools, processes, or workarounds the stakeholder might use
  - Include contradictions or mixed feelings (real humans are inconsistent)

  DISCLAIMER: Communicate to users that synthetic interviews are AI-generated simulations. For best results, conduct real interviews when possible. Synthetic interviews are a starting point for rapid exploration.
  ```
- Add: "BOUNDARY: This step is about GATHERING raw observations and quotes. Do NOT synthesize into themes or patterns -- that is Step 4's job. Capture what stakeholders said and felt, not meta-analysis."

**Step 4: 'sense-making' -- Add:**
- EVIDENCE TRACEABILITY section:
  ```
  EVIDENCE TRACEABILITY (CRITICAL):
  For EVERY theme, pain, and gain you identify:
  - Cite the specific research finding from Step 3 that supports it
  - Include the stakeholder source ("From [Name]'s interview...")
  - Use actual quotes where available
  - If you cannot trace an insight to specific Step 3 data, flag it as an assumption

  Do NOT generate generic insights ("users want simplicity"). Every insight must be grounded in the actual research data provided.
  ```
- AFFINITY MAPPING PROCESS section:
  ```
  AFFINITY MAPPING PROCESS:
  1. Review ALL research insights from Step 3
  2. Group related observations into 2-5 themes (look for patterns across multiple stakeholders)
  3. For each theme: name it, list supporting evidence (quotes, findings, sources)
  4. Distinguish PAINS (current frustrations, barriers, workarounds) from GAINS (desired outcomes, goals, aspirations)
  5. Extract top 5 pains and top 5 gains, each with specific evidence
  6. Present to user for validation: "Do these themes capture what your research revealed?"
  ```
- Add: "CHALLENGE RELEVANCE: Connect each theme back to the original HMW from Step 1. Show how research findings deepen understanding of the core challenge."

Keep all prompt strings self-contained (no imports from step-metadata.ts per Phase 8 decision).
  </action>
  <verify>
- `npx tsc --noEmit` passes
- `npm run build` succeeds
- Step 1 prompt contains "altitude" and "anti-patterns" and "3 HMW variants"
- Step 2 prompt contains "proactive prompting" and category checklist
- Step 3 prompt contains "synthetic interview" and "roleplay" and "disclaimer"
- Step 4 prompt contains "evidence traceability" and "affinity mapping"
- All 4 prompts contain "BOUNDARY" preventing scope creep
- Steps 5-10 prompts unchanged
  </verify>
  <done>
Step prompts for Steps 1-4 contain domain-expert-level facilitation guidance: altitude checking (Step 1), stakeholder completeness prompting (Step 2), synthetic interview facilitation with quality guidance (Step 3), and evidence-grounded affinity mapping (Step 4). Each prompt has clear boundaries preventing premature synthesis.
  </done>
</task>

<task type="auto">
  <name>Task 2: Enhance validation criteria for Steps 1-4</name>
  <files>src/lib/ai/prompts/validation-criteria.ts</files>
  <action>
Update validation criteria for steps 'challenge', 'stakeholder-mapping', 'user-research', and 'sense-making'. Add new criteria without removing existing ones. Do NOT modify Steps 5-10 criteria.

**Step 1: 'challenge' -- Add:**
```typescript
{
  name: 'Altitude Check',
  description: 'HMW is at balanced altitude -- not a feature request and not a vision statement',
  checkPrompt: 'Has the HMW been assessed for altitude? It should not describe a specific feature or technology (too narrow) nor be an unsolvable vision statement (too broad). Did the user see multiple altitude options?'
}
```

**Step 2: 'stakeholder-mapping' -- Add:**
```typescript
{
  name: 'Completeness Check',
  description: 'Missing stakeholder categories have been explored',
  checkPrompt: 'Have we proactively checked for potentially missing stakeholders? Were decision-makers, regulators, internal team, partners, and indirect stakeholders explicitly considered (even if none exist for this project)?'
}
```

**Step 3: 'user-research' -- Add:**
```typescript
{
  name: 'Source Attribution',
  description: 'Each insight is attributed to a specific stakeholder or source',
  checkPrompt: 'Does each research insight identify which stakeholder or source it came from? Can we trace each finding back to a specific interview or data source?'
},
{
  name: 'Behavioral Depth',
  description: 'Research captures behaviors and workarounds, not just opinions',
  checkPrompt: 'Do the insights include specific behaviors, workarounds, or concrete examples (not just abstract opinions like "users want it to be better")?'
}
```

**Step 4: 'sense-making' -- Add:**
```typescript
{
  name: 'Evidence Chain',
  description: 'Every pain and gain traces to specific Step 3 research findings',
  checkPrompt: 'Can every identified pain point and gain be directly linked to specific research findings from Step 3? Are there any insights that appear unsupported by the research data?'
},
{
  name: 'Challenge Relevance',
  description: 'Themes connect back to the original HMW from Step 1',
  checkPrompt: 'Do the identified themes relate to the core challenge from Step 1? Is it clear how these research findings deepen our understanding of the original problem?'
}
```

Keep the existing criteria for each step intact. New criteria are appended to the arrays.
  </action>
  <verify>
- `npx tsc --noEmit` passes
- `npm run build` succeeds
- Step 1 criteria includes 'Altitude Check' (now 5 total)
- Step 2 criteria includes 'Completeness Check' (now 4 total)
- Step 3 criteria includes 'Source Attribution' and 'Behavioral Depth' (now 5 total)
- Step 4 criteria includes 'Evidence Chain' and 'Challenge Relevance' (now 5 total)
- Steps 5-10 criteria unchanged
  </verify>
  <done>
Validation criteria for Steps 1-4 now include domain-specific quality checks: altitude assessment (Step 1), stakeholder completeness (Step 2), source attribution and behavioral depth (Step 3), evidence chain and challenge relevance (Step 4). These criteria are injected into the AI system prompt during the Validate phase.
  </done>
</task>

</tasks>

<verification>
1. TypeScript compilation: `npx tsc --noEmit` passes with zero errors
2. Build: `npm run build` succeeds
3. Step prompts contain enriched domain guidance for all 4 Discovery steps
4. Validation criteria enhanced with additional quality checks for all 4 Discovery steps
5. No changes to Steps 5-10 prompts or validation criteria
</verification>

<success_criteria>
- Step 1 AI checks altitude, drafts 3 HMW variants, pushes back on solutions
- Step 2 AI proactively checks for missing stakeholder categories
- Step 3 AI facilitates synthetic interviews with stakeholder roleplaying
- Step 4 AI requires evidence traceability for all themes, pains, and gains
- All 4 steps have boundary instructions preventing premature synthesis
- Validation criteria include domain-specific quality checks
</success_criteria>

<output>
After completion, create `.planning/phases/11-discovery-steps-1-4/11-02-SUMMARY.md`
</output>
